{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUESS MY WEIGHT \n",
    "\n",
    "![guess_your_weight.gif](images/guess_your_weight.gif)\n",
    "\n",
    "## Table of Contents TOC\n",
    "[Overview](#overview)<br />\n",
    "[Data Understanding](#data-understanding)<br />\n",
    "[Data Preparation](#data-preparation)<br />\n",
    "[Modeling](#modeling)<br />\n",
    "[Evaluation](#evaluation)<br />\n",
    "[Github Repository and Resources](#github-repository-and-resources)<br />\n",
    "\n",
    "\n",
    "## Overview\n",
    "Health and Wellness is a big business. Specifically, weight loss. We’re all trying because it’s very, very hard. I recently went on my own weight loss journey, losing about 50 lbs in roughly 18 months. Weighing myself every morning, I agonized over every tenth of a lb, recording it in an app on my phone. I realized that losing big chunks of weights starts with small, incremental progress on the scale. But I didn’t stop there. As a data nerd I thought, “let’s record every meal.” So I did that too. I wondered… given all this data I have, could I predict my weight? My watch and phone captures my exercise, sleep, eating, and so much more. There must be trends here. At a minimum, I should be able to predict whether my weight will go up or down from the previous day. So let’s do it.<br />\n",
    "[return to TOC](#table-of-contents-TOC)\n",
    "\n",
    "## Data Understanding\n",
    "I have much (and probably too much) of this data in my iphone and Apple Watch. It contains the weight information, workouts, heart rate, meals - broken down into subcategories (proteins, fats, etc). Most importantly is the weight. That will be the feature that I primarily use for classification.  \n",
    "\n",
    "Because it’s my data, there’s more clarity about data entry methods. This is more subjective, than a controlled experiment with many participants. I know what data I was diligent about collecting so I should be able to scrub it appropriately. For instance, I didn’t record my fluids consistently - water, tea, coffee. Water consumption is a big part of this so I’ll have to be clear about the gaps in the data.<br />\n",
    "[return to TOC](#table-of-contents-TOC)\n",
    "\n",
    "\n",
    "## Data Preparation\n",
    "The data is stored on a csv file in a kaggle repository.\n",
    "\n",
    "in an xml file on my phone. After downloading it into python notebook and digging a little, there are roughly 180 rows of weight entries (approximately 6 months) but it’s not clear how many gaps there are. All of the data is stored as an entry, with time stamps and usually some numeric form. Whether it’s heart rate, weight, caloric info, it’s one numeric entry with an associated units. We’re primarily dealing with ints and floats, all numeric, and we’ll be using daily totals/averages. Because we only have one weigh-in per day, we’re only going to use daily values of other data. So… we know we have approximately 100-180 rows. I can’t say at the moment how many columns, because this will be based on what happens in pre-processing. Which brings me to../.\n",
    "\n",
    "There are two major challenges with the pre-processing. The first deals with the privacy of my personal health data. How do I balance reproducibility requirements with privacy concerns? I need to make the dataset publicly available, including all of my pre-processing steps, but I also want to make sure no one can link it back to me, Andrew Q. Bennett (my real middle name doesn’t start with Q… gotcha!!!!). And the initial dataset is large, maybe 40 MB. The approach we’ll use is to perform some pre-processing locally, and then upload to the kaggle site when it’s ready for public consumption. In my jupyter notebook, I will comment out some of this code so that we can see the work, but it won’t affect the code when we press “run”.\n",
    "\n",
    "The second is dealing with correlation efforts. For instance, we know that all data related to working out is going to be correlated with eachother. The steps, average heart rate, workout calories, etc will all be correlated to whether I went for a jog that day. Making decisions about which data to use will be a challenge, even with some baseline domain knowledge. There is a treasure trove that may have nothing (or very little) to do with weight loss, like Vitamin A intake. PCA Analysis will be critical without losing some data. I know about health…but I’m no expert. Maybe Vitamin A intake can help/hurt weight loss.\n",
    "\n",
    "The many visualization efforts will come from making sure the weight data is presented cleanly. A nice, regression line showing weight trends over different periods will be very helpful.<br />\n",
    "[return to TOC](#table-of-contents-TOC)\n",
    "\n",
    "### Instructions for Google Colab\n",
    "Do not run the code snippet below. This is merely a reference if you'd like to download the dataset from Kaggle. Uncomment the below code snippet for downloading datasets from kaggle in Google Colab, the very first time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opendatasets in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (0.1.22)\n",
      "Requirement already satisfied: click in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from opendatasets) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from opendatasets) (4.50.2)\n",
      "Requirement already satisfied: kaggle in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from opendatasets) (1.6.11)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from kaggle->opendatasets) (2024.2.2)\n",
      "Requirement already satisfied: requests in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from kaggle->opendatasets) (2.24.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from kaggle->opendatasets) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\benne\\appdata\\roaming\\python\\python38\\site-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
      "Requirement already satisfied: bleach in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from kaggle->opendatasets) (3.2.1)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from kaggle->opendatasets) (1.25.10)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from kaggle->opendatasets) (8.0.4)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->kaggle->opendatasets) (2.10)\n",
      "Requirement already satisfied: packaging in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from bleach->kaggle->opendatasets) (24.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: kaggle in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (1.6.11)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from kaggle) (2024.2.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from kaggle) (4.50.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\benne\\appdata\\roaming\\python\\python38\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from kaggle) (2.24.0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from kaggle) (1.25.10)\n",
      "Requirement already satisfied: bleach in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from kaggle) (3.2.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->kaggle) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from bleach->kaggle) (24.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\benne\\anaconda3\\envs\\learn-env\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install opendatasets\n",
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \".\\guess-my-weight-4-25\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "import pandas\n",
    " \n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/andrewmbennett/guess-my-weight-4-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/guess-my-weight-4-25/merge_health_4_25.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>BodyMass_lb</th>\n",
       "      <th>StepCount_count</th>\n",
       "      <th>DistanceWalkingRunning_mi</th>\n",
       "      <th>BasalEnergyBurned_Cal</th>\n",
       "      <th>ActiveEnergyBurned_Cal</th>\n",
       "      <th>FlightsClimbed_count</th>\n",
       "      <th>DietaryFatTotal_g</th>\n",
       "      <th>DietaryFatPolyunsaturated_g</th>\n",
       "      <th>DietaryFatMonounsaturated_g</th>\n",
       "      <th>...</th>\n",
       "      <th>DietaryZinc_mg</th>\n",
       "      <th>DietarySelenium_mcg</th>\n",
       "      <th>DietaryCopper_mg</th>\n",
       "      <th>DietaryManganese_mg</th>\n",
       "      <th>DietaryPotassium_mg</th>\n",
       "      <th>AppleExerciseTime_min</th>\n",
       "      <th>SleepAnalysis_AsleepDeep_hrs</th>\n",
       "      <th>SleepAnalysis_AsleepCore_hrs</th>\n",
       "      <th>SleepAnalysis_AsleepREM_hrs</th>\n",
       "      <th>SleepAnalysis_Awake_hrs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08-24</td>\n",
       "      <td>196.9</td>\n",
       "      <td>8895.0</td>\n",
       "      <td>4.163569</td>\n",
       "      <td>2055.322</td>\n",
       "      <td>564.7780</td>\n",
       "      <td>24.0</td>\n",
       "      <td>159.7455</td>\n",
       "      <td>11.8</td>\n",
       "      <td>9.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>5.558333</td>\n",
       "      <td>1.766667</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08-25</td>\n",
       "      <td>195.1</td>\n",
       "      <td>9276.0</td>\n",
       "      <td>4.512434</td>\n",
       "      <td>2174.950</td>\n",
       "      <td>793.3800</td>\n",
       "      <td>7.0</td>\n",
       "      <td>62.9275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.008333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>195.1</td>\n",
       "      <td>10883.0</td>\n",
       "      <td>4.948209</td>\n",
       "      <td>2074.476</td>\n",
       "      <td>395.3870</td>\n",
       "      <td>9.0</td>\n",
       "      <td>118.3000</td>\n",
       "      <td>8.3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>1.558333</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-08-27</td>\n",
       "      <td>192.9</td>\n",
       "      <td>19174.0</td>\n",
       "      <td>9.909258</td>\n",
       "      <td>2187.383</td>\n",
       "      <td>895.4360</td>\n",
       "      <td>14.0</td>\n",
       "      <td>79.9300</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>5.566667</td>\n",
       "      <td>2.591667</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-08-28</td>\n",
       "      <td>192.9</td>\n",
       "      <td>13636.0</td>\n",
       "      <td>6.833914</td>\n",
       "      <td>2186.244</td>\n",
       "      <td>901.5490</td>\n",
       "      <td>21.0</td>\n",
       "      <td>70.8500</td>\n",
       "      <td>4.6</td>\n",
       "      <td>7.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>455.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>5.275000</td>\n",
       "      <td>2.008333</td>\n",
       "      <td>0.158333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>2024-03-04</td>\n",
       "      <td>175.7</td>\n",
       "      <td>8191.0</td>\n",
       "      <td>4.051709</td>\n",
       "      <td>1983.933</td>\n",
       "      <td>499.0720</td>\n",
       "      <td>4.0</td>\n",
       "      <td>87.7000</td>\n",
       "      <td>7.9</td>\n",
       "      <td>9.9</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2024-03-05</td>\n",
       "      <td>174.2</td>\n",
       "      <td>8882.0</td>\n",
       "      <td>4.448750</td>\n",
       "      <td>2009.083</td>\n",
       "      <td>566.5723</td>\n",
       "      <td>9.0</td>\n",
       "      <td>88.6000</td>\n",
       "      <td>4.8</td>\n",
       "      <td>6.3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2387.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>4.775000</td>\n",
       "      <td>1.858333</td>\n",
       "      <td>2.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2024-03-06</td>\n",
       "      <td>173.3</td>\n",
       "      <td>2610.0</td>\n",
       "      <td>1.272886</td>\n",
       "      <td>759.761</td>\n",
       "      <td>127.8580</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2023-08-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7325.0</td>\n",
       "      <td>3.399540</td>\n",
       "      <td>2057.531</td>\n",
       "      <td>476.7400</td>\n",
       "      <td>17.0</td>\n",
       "      <td>80.7000</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>422.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>1.091667</td>\n",
       "      <td>0.241667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2023-08-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>1.941667</td>\n",
       "      <td>0.008333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  BodyMass_lb  StepCount_count  DistanceWalkingRunning_mi  \\\n",
       "0    2023-08-24        196.9           8895.0                   4.163569   \n",
       "1    2023-08-25        195.1           9276.0                   4.512434   \n",
       "2    2023-08-26        195.1          10883.0                   4.948209   \n",
       "3    2023-08-27        192.9          19174.0                   9.909258   \n",
       "4    2023-08-28        192.9          13636.0                   6.833914   \n",
       "..          ...          ...              ...                        ...   \n",
       "193  2024-03-04        175.7           8191.0                   4.051709   \n",
       "194  2024-03-05        174.2           8882.0                   4.448750   \n",
       "195  2024-03-06        173.3           2610.0                   1.272886   \n",
       "196  2023-08-23          NaN           7325.0                   3.399540   \n",
       "197  2023-08-22          NaN              NaN                        NaN   \n",
       "\n",
       "     BasalEnergyBurned_Cal  ActiveEnergyBurned_Cal  FlightsClimbed_count  \\\n",
       "0                 2055.322                564.7780                  24.0   \n",
       "1                 2174.950                793.3800                   7.0   \n",
       "2                 2074.476                395.3870                   9.0   \n",
       "3                 2187.383                895.4360                  14.0   \n",
       "4                 2186.244                901.5490                  21.0   \n",
       "..                     ...                     ...                   ...   \n",
       "193               1983.933                499.0720                   4.0   \n",
       "194               2009.083                566.5723                   9.0   \n",
       "195                759.761                127.8580                   2.0   \n",
       "196               2057.531                476.7400                  17.0   \n",
       "197                    NaN                     NaN                   NaN   \n",
       "\n",
       "     DietaryFatTotal_g  DietaryFatPolyunsaturated_g  \\\n",
       "0             159.7455                         11.8   \n",
       "1              62.9275                          0.0   \n",
       "2             118.3000                          8.3   \n",
       "3              79.9300                          3.1   \n",
       "4              70.8500                          4.6   \n",
       "..                 ...                          ...   \n",
       "193            87.7000                          7.9   \n",
       "194            88.6000                          4.8   \n",
       "195                NaN                          NaN   \n",
       "196            80.7000                          1.8   \n",
       "197                NaN                          NaN   \n",
       "\n",
       "     DietaryFatMonounsaturated_g  ...  DietaryZinc_mg  DietarySelenium_mcg  \\\n",
       "0                            9.5  ...             0.5                  9.0   \n",
       "1                            0.0  ...             0.0                  0.0   \n",
       "2                           15.0  ...             1.4                 13.0   \n",
       "3                            2.9  ...             1.5                 18.0   \n",
       "4                            7.1  ...             1.3                 17.0   \n",
       "..                           ...  ...             ...                  ...   \n",
       "193                          9.9  ...             2.8                 58.0   \n",
       "194                          6.3  ...             1.9                123.0   \n",
       "195                          NaN  ...             NaN                  NaN   \n",
       "196                          0.9  ...             0.5                  2.0   \n",
       "197                          NaN  ...             NaN                  NaN   \n",
       "\n",
       "     DietaryCopper_mg  DietaryManganese_mg  DietaryPotassium_mg  \\\n",
       "0                 0.3                  1.1               1572.0   \n",
       "1                 0.0                  0.0                  0.0   \n",
       "2                 0.5                  0.8               1943.0   \n",
       "3                 0.3                  0.5               1986.0   \n",
       "4                 0.3                  0.9                455.0   \n",
       "..                ...                  ...                  ...   \n",
       "193               0.3                  2.8               1023.0   \n",
       "194               0.1                  0.8               2387.0   \n",
       "195               NaN                  NaN                  NaN   \n",
       "196               0.2                  0.5                422.0   \n",
       "197               NaN                  NaN                  NaN   \n",
       "\n",
       "     AppleExerciseTime_min  SleepAnalysis_AsleepDeep_hrs  \\\n",
       "0                     12.0                      0.783333   \n",
       "1                     36.0                      1.008333   \n",
       "2                      8.0                      1.400000   \n",
       "3                     45.0                      0.891667   \n",
       "4                     43.0                      0.641667   \n",
       "..                     ...                           ...   \n",
       "193                   76.0                      0.000000   \n",
       "194                  135.0                      0.816667   \n",
       "195                    3.0                           NaN   \n",
       "196                   10.0                      0.983333   \n",
       "197                    NaN                      0.650000   \n",
       "\n",
       "     SleepAnalysis_AsleepCore_hrs  SleepAnalysis_AsleepREM_hrs  \\\n",
       "0                        5.558333                     1.766667   \n",
       "1                        3.700000                     1.500000   \n",
       "2                        3.916667                     1.558333   \n",
       "3                        5.566667                     2.591667   \n",
       "4                        5.275000                     2.008333   \n",
       "..                            ...                          ...   \n",
       "193                      0.000000                     0.000000   \n",
       "194                      4.775000                     1.858333   \n",
       "195                           NaN                          NaN   \n",
       "196                      3.400000                     1.091667   \n",
       "197                      4.833333                     1.941667   \n",
       "\n",
       "     SleepAnalysis_Awake_hrs  \n",
       "0                   0.266667  \n",
       "1                   0.133333  \n",
       "2                   0.050000  \n",
       "3                   0.066667  \n",
       "4                   0.158333  \n",
       "..                       ...  \n",
       "193                 0.000000  \n",
       "194                 2.683333  \n",
       "195                      NaN  \n",
       "196                 0.241667  \n",
       "197                 0.008333  \n",
       "\n",
       "[198 rows x 46 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things will do is make the date the index and convert date to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Column Feature and Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the date\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>BodyMass_lb</th>\n",
       "      <th>StepCount_count</th>\n",
       "      <th>DistanceWalkingRunning_mi</th>\n",
       "      <th>BasalEnergyBurned_Cal</th>\n",
       "      <th>ActiveEnergyBurned_Cal</th>\n",
       "      <th>FlightsClimbed_count</th>\n",
       "      <th>DietaryFatTotal_g</th>\n",
       "      <th>DietaryFatPolyunsaturated_g</th>\n",
       "      <th>DietaryFatMonounsaturated_g</th>\n",
       "      <th>...</th>\n",
       "      <th>DietarySelenium_mcg</th>\n",
       "      <th>DietaryCopper_mg</th>\n",
       "      <th>DietaryManganese_mg</th>\n",
       "      <th>DietaryPotassium_mg</th>\n",
       "      <th>AppleExerciseTime_min</th>\n",
       "      <th>SleepAnalysis_AsleepDeep_hrs</th>\n",
       "      <th>SleepAnalysis_AsleepCore_hrs</th>\n",
       "      <th>SleepAnalysis_AsleepREM_hrs</th>\n",
       "      <th>SleepAnalysis_Awake_hrs</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08-24</td>\n",
       "      <td>196.9</td>\n",
       "      <td>8895.0</td>\n",
       "      <td>4.163569</td>\n",
       "      <td>2055.322</td>\n",
       "      <td>564.7780</td>\n",
       "      <td>24.0</td>\n",
       "      <td>159.7455</td>\n",
       "      <td>11.8</td>\n",
       "      <td>9.5</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>5.558333</td>\n",
       "      <td>1.766667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08-25</td>\n",
       "      <td>195.1</td>\n",
       "      <td>9276.0</td>\n",
       "      <td>4.512434</td>\n",
       "      <td>2174.950</td>\n",
       "      <td>793.3800</td>\n",
       "      <td>7.0</td>\n",
       "      <td>62.9275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.008333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>195.1</td>\n",
       "      <td>10883.0</td>\n",
       "      <td>4.948209</td>\n",
       "      <td>2074.476</td>\n",
       "      <td>395.3870</td>\n",
       "      <td>9.0</td>\n",
       "      <td>118.3000</td>\n",
       "      <td>8.3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>1.558333</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-08-27</td>\n",
       "      <td>192.9</td>\n",
       "      <td>19174.0</td>\n",
       "      <td>9.909258</td>\n",
       "      <td>2187.383</td>\n",
       "      <td>895.4360</td>\n",
       "      <td>14.0</td>\n",
       "      <td>79.9300</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>5.566667</td>\n",
       "      <td>2.591667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-08-28</td>\n",
       "      <td>192.9</td>\n",
       "      <td>13636.0</td>\n",
       "      <td>6.833914</td>\n",
       "      <td>2186.244</td>\n",
       "      <td>901.5490</td>\n",
       "      <td>21.0</td>\n",
       "      <td>70.8500</td>\n",
       "      <td>4.6</td>\n",
       "      <td>7.1</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>455.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>5.275000</td>\n",
       "      <td>2.008333</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>2024-03-04</td>\n",
       "      <td>175.7</td>\n",
       "      <td>8191.0</td>\n",
       "      <td>4.051709</td>\n",
       "      <td>1983.933</td>\n",
       "      <td>499.0720</td>\n",
       "      <td>4.0</td>\n",
       "      <td>87.7000</td>\n",
       "      <td>7.9</td>\n",
       "      <td>9.9</td>\n",
       "      <td>...</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2024-03-05</td>\n",
       "      <td>174.2</td>\n",
       "      <td>8882.0</td>\n",
       "      <td>4.448750</td>\n",
       "      <td>2009.083</td>\n",
       "      <td>566.5723</td>\n",
       "      <td>9.0</td>\n",
       "      <td>88.6000</td>\n",
       "      <td>4.8</td>\n",
       "      <td>6.3</td>\n",
       "      <td>...</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2387.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>4.775000</td>\n",
       "      <td>1.858333</td>\n",
       "      <td>2.683333</td>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2024-03-06</td>\n",
       "      <td>173.3</td>\n",
       "      <td>2610.0</td>\n",
       "      <td>1.272886</td>\n",
       "      <td>759.761</td>\n",
       "      <td>127.8580</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2023-08-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7325.0</td>\n",
       "      <td>3.399540</td>\n",
       "      <td>2057.531</td>\n",
       "      <td>476.7400</td>\n",
       "      <td>17.0</td>\n",
       "      <td>80.7000</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>422.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>1.091667</td>\n",
       "      <td>0.241667</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2023-08-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>1.941667</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  BodyMass_lb  StepCount_count  DistanceWalkingRunning_mi  \\\n",
       "0   2023-08-24        196.9           8895.0                   4.163569   \n",
       "1   2023-08-25        195.1           9276.0                   4.512434   \n",
       "2   2023-08-26        195.1          10883.0                   4.948209   \n",
       "3   2023-08-27        192.9          19174.0                   9.909258   \n",
       "4   2023-08-28        192.9          13636.0                   6.833914   \n",
       "..         ...          ...              ...                        ...   \n",
       "193 2024-03-04        175.7           8191.0                   4.051709   \n",
       "194 2024-03-05        174.2           8882.0                   4.448750   \n",
       "195 2024-03-06        173.3           2610.0                   1.272886   \n",
       "196 2023-08-23          NaN           7325.0                   3.399540   \n",
       "197 2023-08-22          NaN              NaN                        NaN   \n",
       "\n",
       "     BasalEnergyBurned_Cal  ActiveEnergyBurned_Cal  FlightsClimbed_count  \\\n",
       "0                 2055.322                564.7780                  24.0   \n",
       "1                 2174.950                793.3800                   7.0   \n",
       "2                 2074.476                395.3870                   9.0   \n",
       "3                 2187.383                895.4360                  14.0   \n",
       "4                 2186.244                901.5490                  21.0   \n",
       "..                     ...                     ...                   ...   \n",
       "193               1983.933                499.0720                   4.0   \n",
       "194               2009.083                566.5723                   9.0   \n",
       "195                759.761                127.8580                   2.0   \n",
       "196               2057.531                476.7400                  17.0   \n",
       "197                    NaN                     NaN                   NaN   \n",
       "\n",
       "     DietaryFatTotal_g  DietaryFatPolyunsaturated_g  \\\n",
       "0             159.7455                         11.8   \n",
       "1              62.9275                          0.0   \n",
       "2             118.3000                          8.3   \n",
       "3              79.9300                          3.1   \n",
       "4              70.8500                          4.6   \n",
       "..                 ...                          ...   \n",
       "193            87.7000                          7.9   \n",
       "194            88.6000                          4.8   \n",
       "195                NaN                          NaN   \n",
       "196            80.7000                          1.8   \n",
       "197                NaN                          NaN   \n",
       "\n",
       "     DietaryFatMonounsaturated_g  ...  DietarySelenium_mcg  DietaryCopper_mg  \\\n",
       "0                            9.5  ...                  9.0               0.3   \n",
       "1                            0.0  ...                  0.0               0.0   \n",
       "2                           15.0  ...                 13.0               0.5   \n",
       "3                            2.9  ...                 18.0               0.3   \n",
       "4                            7.1  ...                 17.0               0.3   \n",
       "..                           ...  ...                  ...               ...   \n",
       "193                          9.9  ...                 58.0               0.3   \n",
       "194                          6.3  ...                123.0               0.1   \n",
       "195                          NaN  ...                  NaN               NaN   \n",
       "196                          0.9  ...                  2.0               0.2   \n",
       "197                          NaN  ...                  NaN               NaN   \n",
       "\n",
       "     DietaryManganese_mg  DietaryPotassium_mg  AppleExerciseTime_min  \\\n",
       "0                    1.1               1572.0                   12.0   \n",
       "1                    0.0                  0.0                   36.0   \n",
       "2                    0.8               1943.0                    8.0   \n",
       "3                    0.5               1986.0                   45.0   \n",
       "4                    0.9                455.0                   43.0   \n",
       "..                   ...                  ...                    ...   \n",
       "193                  2.8               1023.0                   76.0   \n",
       "194                  0.8               2387.0                  135.0   \n",
       "195                  NaN                  NaN                    3.0   \n",
       "196                  0.5                422.0                   10.0   \n",
       "197                  NaN                  NaN                    NaN   \n",
       "\n",
       "     SleepAnalysis_AsleepDeep_hrs  SleepAnalysis_AsleepCore_hrs  \\\n",
       "0                        0.783333                      5.558333   \n",
       "1                        1.008333                      3.700000   \n",
       "2                        1.400000                      3.916667   \n",
       "3                        0.891667                      5.566667   \n",
       "4                        0.641667                      5.275000   \n",
       "..                            ...                           ...   \n",
       "193                      0.000000                      0.000000   \n",
       "194                      0.816667                      4.775000   \n",
       "195                           NaN                           NaN   \n",
       "196                      0.983333                      3.400000   \n",
       "197                      0.650000                      4.833333   \n",
       "\n",
       "     SleepAnalysis_AsleepREM_hrs  SleepAnalysis_Awake_hrs        day  \n",
       "0                       1.766667                 0.266667   Thursday  \n",
       "1                       1.500000                 0.133333     Friday  \n",
       "2                       1.558333                 0.050000   Saturday  \n",
       "3                       2.591667                 0.066667     Sunday  \n",
       "4                       2.008333                 0.158333     Monday  \n",
       "..                           ...                      ...        ...  \n",
       "193                     0.000000                 0.000000     Monday  \n",
       "194                     1.858333                 2.683333    Tuesday  \n",
       "195                          NaN                      NaN  Wednesday  \n",
       "196                     1.091667                 0.241667  Wednesday  \n",
       "197                     1.941667                 0.008333    Tuesday  \n",
       "\n",
       "[198 rows x 47 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['day'] = df['date'].dt.day_name()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Date the index \n",
    "df.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the last row \n",
    "df.drop(['2023-08-22', '2023-08-23'], axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BodyMass_lb</th>\n",
       "      <th>StepCount_count</th>\n",
       "      <th>DistanceWalkingRunning_mi</th>\n",
       "      <th>BasalEnergyBurned_Cal</th>\n",
       "      <th>ActiveEnergyBurned_Cal</th>\n",
       "      <th>FlightsClimbed_count</th>\n",
       "      <th>DietaryFatTotal_g</th>\n",
       "      <th>DietaryFatPolyunsaturated_g</th>\n",
       "      <th>DietaryFatMonounsaturated_g</th>\n",
       "      <th>DietaryFatSaturated_g</th>\n",
       "      <th>...</th>\n",
       "      <th>DietarySelenium_mcg</th>\n",
       "      <th>DietaryCopper_mg</th>\n",
       "      <th>DietaryManganese_mg</th>\n",
       "      <th>DietaryPotassium_mg</th>\n",
       "      <th>AppleExerciseTime_min</th>\n",
       "      <th>SleepAnalysis_AsleepDeep_hrs</th>\n",
       "      <th>SleepAnalysis_AsleepCore_hrs</th>\n",
       "      <th>SleepAnalysis_AsleepREM_hrs</th>\n",
       "      <th>SleepAnalysis_Awake_hrs</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-08-24</th>\n",
       "      <td>196.9</td>\n",
       "      <td>8895.0</td>\n",
       "      <td>4.163569</td>\n",
       "      <td>2055.322</td>\n",
       "      <td>564.7780</td>\n",
       "      <td>24.0</td>\n",
       "      <td>159.7455</td>\n",
       "      <td>11.8</td>\n",
       "      <td>9.5</td>\n",
       "      <td>36.2203</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>5.558333</td>\n",
       "      <td>1.766667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-25</th>\n",
       "      <td>195.1</td>\n",
       "      <td>9276.0</td>\n",
       "      <td>4.512434</td>\n",
       "      <td>2174.950</td>\n",
       "      <td>793.3800</td>\n",
       "      <td>7.0</td>\n",
       "      <td>62.9275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.8165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.008333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-26</th>\n",
       "      <td>195.1</td>\n",
       "      <td>10883.0</td>\n",
       "      <td>4.948209</td>\n",
       "      <td>2074.476</td>\n",
       "      <td>395.3870</td>\n",
       "      <td>9.0</td>\n",
       "      <td>118.3000</td>\n",
       "      <td>8.3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>39.5000</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>1.558333</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-27</th>\n",
       "      <td>192.9</td>\n",
       "      <td>19174.0</td>\n",
       "      <td>9.909258</td>\n",
       "      <td>2187.383</td>\n",
       "      <td>895.4360</td>\n",
       "      <td>14.0</td>\n",
       "      <td>79.9300</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>27.9600</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>5.566667</td>\n",
       "      <td>2.591667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-28</th>\n",
       "      <td>192.9</td>\n",
       "      <td>13636.0</td>\n",
       "      <td>6.833914</td>\n",
       "      <td>2186.244</td>\n",
       "      <td>901.5490</td>\n",
       "      <td>21.0</td>\n",
       "      <td>70.8500</td>\n",
       "      <td>4.6</td>\n",
       "      <td>7.1</td>\n",
       "      <td>16.3000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>455.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>5.275000</td>\n",
       "      <td>2.008333</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-02</th>\n",
       "      <td>174.6</td>\n",
       "      <td>13416.0</td>\n",
       "      <td>6.533640</td>\n",
       "      <td>2048.925</td>\n",
       "      <td>1651.9890</td>\n",
       "      <td>16.0</td>\n",
       "      <td>76.2000</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-03</th>\n",
       "      <td>175.0</td>\n",
       "      <td>15876.0</td>\n",
       "      <td>7.722016</td>\n",
       "      <td>2048.189</td>\n",
       "      <td>1443.2150</td>\n",
       "      <td>22.0</td>\n",
       "      <td>59.9000</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>9.7000</td>\n",
       "      <td>...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>1.108333</td>\n",
       "      <td>3.925000</td>\n",
       "      <td>1.966667</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-04</th>\n",
       "      <td>175.7</td>\n",
       "      <td>8191.0</td>\n",
       "      <td>4.051709</td>\n",
       "      <td>1983.933</td>\n",
       "      <td>499.0720</td>\n",
       "      <td>4.0</td>\n",
       "      <td>87.7000</td>\n",
       "      <td>7.9</td>\n",
       "      <td>9.9</td>\n",
       "      <td>25.9000</td>\n",
       "      <td>...</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-05</th>\n",
       "      <td>174.2</td>\n",
       "      <td>8882.0</td>\n",
       "      <td>4.448750</td>\n",
       "      <td>2009.083</td>\n",
       "      <td>566.5723</td>\n",
       "      <td>9.0</td>\n",
       "      <td>88.6000</td>\n",
       "      <td>4.8</td>\n",
       "      <td>6.3</td>\n",
       "      <td>21.8000</td>\n",
       "      <td>...</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2387.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>4.775000</td>\n",
       "      <td>1.858333</td>\n",
       "      <td>2.683333</td>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-06</th>\n",
       "      <td>173.3</td>\n",
       "      <td>2610.0</td>\n",
       "      <td>1.272886</td>\n",
       "      <td>759.761</td>\n",
       "      <td>127.8580</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            BodyMass_lb  StepCount_count  DistanceWalkingRunning_mi  \\\n",
       "date                                                                  \n",
       "2023-08-24        196.9           8895.0                   4.163569   \n",
       "2023-08-25        195.1           9276.0                   4.512434   \n",
       "2023-08-26        195.1          10883.0                   4.948209   \n",
       "2023-08-27        192.9          19174.0                   9.909258   \n",
       "2023-08-28        192.9          13636.0                   6.833914   \n",
       "...                 ...              ...                        ...   \n",
       "2024-03-02        174.6          13416.0                   6.533640   \n",
       "2024-03-03        175.0          15876.0                   7.722016   \n",
       "2024-03-04        175.7           8191.0                   4.051709   \n",
       "2024-03-05        174.2           8882.0                   4.448750   \n",
       "2024-03-06        173.3           2610.0                   1.272886   \n",
       "\n",
       "            BasalEnergyBurned_Cal  ActiveEnergyBurned_Cal  \\\n",
       "date                                                        \n",
       "2023-08-24               2055.322                564.7780   \n",
       "2023-08-25               2174.950                793.3800   \n",
       "2023-08-26               2074.476                395.3870   \n",
       "2023-08-27               2187.383                895.4360   \n",
       "2023-08-28               2186.244                901.5490   \n",
       "...                           ...                     ...   \n",
       "2024-03-02               2048.925               1651.9890   \n",
       "2024-03-03               2048.189               1443.2150   \n",
       "2024-03-04               1983.933                499.0720   \n",
       "2024-03-05               2009.083                566.5723   \n",
       "2024-03-06                759.761                127.8580   \n",
       "\n",
       "            FlightsClimbed_count  DietaryFatTotal_g  \\\n",
       "date                                                  \n",
       "2023-08-24                  24.0           159.7455   \n",
       "2023-08-25                   7.0            62.9275   \n",
       "2023-08-26                   9.0           118.3000   \n",
       "2023-08-27                  14.0            79.9300   \n",
       "2023-08-28                  21.0            70.8500   \n",
       "...                          ...                ...   \n",
       "2024-03-02                  16.0            76.2000   \n",
       "2024-03-03                  22.0            59.9000   \n",
       "2024-03-04                   4.0            87.7000   \n",
       "2024-03-05                   9.0            88.6000   \n",
       "2024-03-06                   2.0                NaN   \n",
       "\n",
       "            DietaryFatPolyunsaturated_g  DietaryFatMonounsaturated_g  \\\n",
       "date                                                                   \n",
       "2023-08-24                         11.8                          9.5   \n",
       "2023-08-25                          0.0                          0.0   \n",
       "2023-08-26                          8.3                         15.0   \n",
       "2023-08-27                          3.1                          2.9   \n",
       "2023-08-28                          4.6                          7.1   \n",
       "...                                 ...                          ...   \n",
       "2024-03-02                          2.4                          3.0   \n",
       "2024-03-03                          1.3                          0.6   \n",
       "2024-03-04                          7.9                          9.9   \n",
       "2024-03-05                          4.8                          6.3   \n",
       "2024-03-06                          NaN                          NaN   \n",
       "\n",
       "            DietaryFatSaturated_g  ...  DietarySelenium_mcg  DietaryCopper_mg  \\\n",
       "date                               ...                                          \n",
       "2023-08-24                36.2203  ...                  9.0               0.3   \n",
       "2023-08-25                10.8165  ...                  0.0               0.0   \n",
       "2023-08-26                39.5000  ...                 13.0               0.5   \n",
       "2023-08-27                27.9600  ...                 18.0               0.3   \n",
       "2023-08-28                16.3000  ...                 17.0               0.3   \n",
       "...                           ...  ...                  ...               ...   \n",
       "2024-03-02                25.0000  ...                 24.0               0.3   \n",
       "2024-03-03                 9.7000  ...                 51.0               0.4   \n",
       "2024-03-04                25.9000  ...                 58.0               0.3   \n",
       "2024-03-05                21.8000  ...                123.0               0.1   \n",
       "2024-03-06                    NaN  ...                  NaN               NaN   \n",
       "\n",
       "            DietaryManganese_mg  DietaryPotassium_mg  AppleExerciseTime_min  \\\n",
       "date                                                                          \n",
       "2023-08-24                  1.1               1572.0                   12.0   \n",
       "2023-08-25                  0.0                  0.0                   36.0   \n",
       "2023-08-26                  0.8               1943.0                    8.0   \n",
       "2023-08-27                  0.5               1986.0                   45.0   \n",
       "2023-08-28                  0.9                455.0                   43.0   \n",
       "...                         ...                  ...                    ...   \n",
       "2024-03-02                  1.4               1996.0                  148.0   \n",
       "2024-03-03                  3.2               1987.0                  173.0   \n",
       "2024-03-04                  2.8               1023.0                   76.0   \n",
       "2024-03-05                  0.8               2387.0                  135.0   \n",
       "2024-03-06                  NaN                  NaN                    3.0   \n",
       "\n",
       "            SleepAnalysis_AsleepDeep_hrs  SleepAnalysis_AsleepCore_hrs  \\\n",
       "date                                                                     \n",
       "2023-08-24                      0.783333                      5.558333   \n",
       "2023-08-25                      1.008333                      3.700000   \n",
       "2023-08-26                      1.400000                      3.916667   \n",
       "2023-08-27                      0.891667                      5.566667   \n",
       "2023-08-28                      0.641667                      5.275000   \n",
       "...                                  ...                           ...   \n",
       "2024-03-02                      0.000000                      0.000000   \n",
       "2024-03-03                      1.108333                      3.925000   \n",
       "2024-03-04                      0.000000                      0.000000   \n",
       "2024-03-05                      0.816667                      4.775000   \n",
       "2024-03-06                           NaN                           NaN   \n",
       "\n",
       "            SleepAnalysis_AsleepREM_hrs  SleepAnalysis_Awake_hrs        day  \n",
       "date                                                                         \n",
       "2023-08-24                     1.766667                 0.266667   Thursday  \n",
       "2023-08-25                     1.500000                 0.133333     Friday  \n",
       "2023-08-26                     1.558333                 0.050000   Saturday  \n",
       "2023-08-27                     2.591667                 0.066667     Sunday  \n",
       "2023-08-28                     2.008333                 0.158333     Monday  \n",
       "...                                 ...                      ...        ...  \n",
       "2024-03-02                     0.000000                 0.000000   Saturday  \n",
       "2024-03-03                     1.966667                 0.300000     Sunday  \n",
       "2024-03-04                     0.000000                 0.000000     Monday  \n",
       "2024-03-05                     1.858333                 2.683333    Tuesday  \n",
       "2024-03-06                          NaN                      NaN  Wednesday  \n",
       "\n",
       "[196 rows x 46 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BodyMass Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    196.000000\n",
       "mean     127.684184\n",
       "std       88.425096\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%      181.100000\n",
       "75%      188.300000\n",
       "max      388.500000\n",
       "Name: BodyMass_lb, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['BodyMass_lb'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df['BodyMass_lb'].plot(figsize = (16,7));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_weights = len(df[df['BodyMass_lb'] < 100])\n",
    "total = len(df['BodyMass_lb'])\n",
    "null_weights/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a few issues to resolve. The biggest issue is the number of zero entries. Based on our knowledge of human weight fluctation, we know it's impossible to weight 0 pounds. More than likely, these are the dates when a wiegh-in was never performed. We should convert these values to NaN to make our graph appear better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#df[df['BodyMass_lb'] == 0]['BodyMass_lb'] = np.NaN \n",
    "#df['BodyMass_lb'].replace(0.0,np.NaN)\n",
    "df.loc[df['BodyMass_lb'] == 0.0,'BodyMass_lb'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BodyMass_lb'].plot(figsize = (16,6));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['BodyMass_lb'] == 388.5,'BodyMass_lb'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BodyMass_lb'].plot(figsize = (16,6));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that we don't have much data related to weight prior to late August. When we go into startdate, it might be prudent to consider Aug. 24th as the actual start date. I'm going to modify the data so there's nothing prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Column inspection - Sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_sleep = ['SleepAnalysis_AsleepDeep_hrs', 'SleepAnalysis_AsleepCore_hrs', 'SleepAnalysis_AsleepREM_hrs', 'SleepAnalysis_Awake_hrs', 'AppleExerciseTime_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(15,7)\n",
    "\n",
    "plt.plot(df['SleepAnalysis_AsleepCore_hrs'], color='blue', label = 'Core')\n",
    "plt.plot(df['SleepAnalysis_AsleepREM_hrs'], color='red', label = 'REM')\n",
    "plt.plot(df['SleepAnalysis_AsleepDeep_hrs'], color='green', label = 'Deep')\n",
    "plt.plot(df['SleepAnalysis_Awake_hrs'], color='yellow', label = 'Awake')\n",
    " \n",
    "plt.title('Sleep_hrs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SleepAnalysis_AsleepDeep_hrs'].replace(to_replace=0,value = df['SleepAnalysis_AsleepDeep_hrs'].mean(), inplace = True)\n",
    "df['SleepAnalysis_AsleepCore_hrs'].replace(to_replace=0,value = df['SleepAnalysis_AsleepCore_hrs'].mean(), inplace = True)\n",
    "df['SleepAnalysis_AsleepREM_hrs'].replace(to_replace=0,value = df['SleepAnalysis_AsleepREM_hrs'].mean(), inplace = True)\n",
    "df['SleepAnalysis_Awake_hrs'].replace(to_replace=0,value = df['SleepAnalysis_Awake_hrs'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[col_sleep] = df[col_sleep].fillna(df[col_sleep].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(15,7)\n",
    "\n",
    "plt.plot(df['SleepAnalysis_AsleepDeep_hrs'], color='green')\n",
    "plt.plot(df['SleepAnalysis_AsleepCore_hrs'], color='blue')\n",
    "plt.plot(df['SleepAnalysis_AsleepREM_hrs'], color='red')\n",
    "plt.plot(df['SleepAnalysis_Awake_hrs'], color='yellow')\n",
    " \n",
    "plt.title('Sleep')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Column inspection - Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_exercise = ['StepCount_count', 'DistanceWalkingRunning_mi', 'BasalEnergyBurned_Cal', 'ActiveEnergyBurned_Cal', 'FlightsClimbed_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['StepCount_count', 'DistanceWalkingRunning_mi', 'FlightsClimbed_count'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(15,7)\n",
    "\n",
    "#plt.plot(df['StepCount_count'], color='green')\n",
    "#plt.plot(df['DistanceWalkingRunning_mi'], color='blue')\n",
    "plt.plot(df['BasalEnergyBurned_Cal'], color='red')\n",
    "plt.plot(df['ActiveEnergyBurned_Cal'], color='yellow')\n",
    "#plt.plot(df['FlightsClimbed_count'], color='yellow')\n",
    " \n",
    "plt.title('Exercise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cond_act = df['ActiveEnergyBurned_Cal'] < 250\n",
    "#bas_act = df['BasalEnergyBurned_Cal'] < 250\n",
    "\n",
    "df.loc[df['ActiveEnergyBurned_Cal'] < 250, 'ActiveEnergyBurned_Cal'] = df['ActiveEnergyBurned_Cal'].mean()\n",
    "df.loc[df['BasalEnergyBurned_Cal'] < 1750, 'BasalEnergyBurned_Cal'] = df['BasalEnergyBurned_Cal'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(15,7)\n",
    "\n",
    "#plt.plot(df['StepCount_count'], color='green')\n",
    "#plt.plot(df['DistanceWalkingRunning_mi'], color='blue')\n",
    "plt.plot(df['BasalEnergyBurned_Cal'], color='red', label = 'Basal')\n",
    "plt.plot(df['ActiveEnergyBurned_Cal'], color='yellow', label = 'Active')\n",
    "#plt.plot(df['FlightsClimbed_count'], color='yellow')\n",
    " \n",
    "plt.title('Exercise Calories')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Column inspection - Dietary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DietaryCarbohydrates_g'].hist(figsize = (16,6), width = 25);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_carbs = len(df[df['DietaryCarbohydrates_g'] == 0])\n",
    "null_carbs/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OKay, so we have considerable 0 values here. \n",
    "\n",
    "Missing at Random (MAR): Data points are missing depending on observed values in other variables, but not on the missing values themselves. This is a more complex scenario, but imputation using observed data can still be effective.\n",
    "\n",
    "After doing some previewing, I'm determining that those 3 data points, whose carbs are under 75g, are also incomplete. So this isn't just the carbohydrate data, but all of the dietary information. So, we'll set all of the dietary information to Nan where the daily carbohydrates are less than 75g.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'll go ahead and limit the \n",
    "Nan_cond = df['DietaryCarbohydrates_g'] < 75.0\n",
    "\n",
    "col_dietary = [col for col in df.columns if \"Dietary\" in col]\n",
    "df.loc[df['DietaryCarbohydrates_g'] < 75.0, col_dietary] = np.nan\n",
    "\n",
    "\n",
    "#df.loc[df['DietaryCarbohydrates_g'] < 75.0,'DietaryCarbohydrates_g'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2023-08-29':'2023-09-04','DietaryFatTotal_g':'DietaryProtein_g']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's figure out which columns we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's plot our carbohydrates\n",
    "df.groupby(['day'])['DietaryCarbohydrates_g'].plot(figsize = (13,8), subplots=False, legend=True);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's plot our carbohydrates\n",
    "df['DietaryCarbohydrates_g'].plot(figsize = (13,8), subplots=False, legend=True);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay, so we have some gaps to fill, let's start with filling in some "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nul_carbs = pd.isnull(df['DietaryCarbohydrates_g'])\n",
    "df[nul_carbs]['DietaryCarbohydrates_g']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick scan here shows that we have chunks of time series data missing. The best way to handle this, in my opion, is to divide into two subsets, to disregard those stretches of missing data.\n",
    "\n",
    "These chunks will be (8-23: 10-24), (10-31: 12-23), (1-01: 02-05), (02-22: 03-05). These were chunks of data were determined by finding \"chunks\" of both null and valid data. Chunks of valid data were determined to have no more than 3 consecutive days of null data. To fill these in, let's start create the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SepOct = df['2023-08-23':'2023-10-24']\n",
    "NovDec = df['2023-10-31':'2023-12-23']\n",
    "Jan = df['2024-01-01':'2024-02-05']\n",
    "FebMar = df['2024-02-24':'2024-03-05']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay, now let's fill in the null values with the mean for all of the dietary nulls here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[col_dietary] = df[col_dietary].fillna(df[col_dietary].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so this looks promising. We see a little flattening of the curve, but, it doesn't mess with our data too much. Let's go ahead and create this for all of our data, in Sept, Oct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(15,7)\n",
    "\n",
    "plt.plot(df['DietaryCarbohydrates_g'], color='green', label = 'Carbs')\n",
    "plt.plot(df['DietaryProtein_g'], color='red', label = 'Protein')\n",
    "plt.plot(df['DietaryFatTotal_g'], color='blue', label = 'Fats')\n",
    " \n",
    "plt.title('Diet Macros (g)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing on Weight\n",
    "What we really care about is weight, and the difference of weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for Stationality\n",
    "First, let's use a Dickey-Fuller Test on our data to see if we have Stationality. We're going to use the Dickey-Fuller test in the stats model. This function does not permit null values. And, we have some null values, so we'll have to fill in missing data. To do this, we'll need to utilize a certain level of synthetic data. For starters, let's just first look at our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(15,7)\n",
    "\n",
    "plt.plot(df['BodyMass_lb'], color='green', label = 'BodyMass_lb')\n",
    " \n",
    "plt.title('Weigh-In Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BodyMass_lb_inter'] = df['BodyMass_lb'].interpolate(option='spline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "plt.rcParams['figure.figsize']=(15,7)\n",
    "\n",
    "plt.plot(df['BodyMass_lb_inter'], color='blue', label = 'Interpolated_Data')\n",
    "plt.plot(df['BodyMass_lb'], color='blue', label = 'Actual')\n",
    "\n",
    "#plt.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%b\"))\n",
    "\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "plt.rc('axes', titlesize=SMALL_SIZE, labelsize=MEDIUM_SIZE)\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "#plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\n",
    "#plt.rc('xtick', labelsize=BIGGER_SIZE)    # fontsize of the tick labels\n",
    "#plt.rc('ytick', labelsize=BIGGER_SIZE)    # fontsize of the tick labels\n",
    "#plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "#plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "#matplotlib.rc('font', size=BIGGER_SIZE)\n",
    "#matplotlib.rc('axes', titlesize=BIGGER_SIZE)\n",
    "\n",
    "plt.title('Daily Weigh-in Data')\n",
    "plt.xlabel('Dates (Yr-Mo)')\n",
    "plt.ylabel('Lbs')\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize a blank series series without the date index\n",
    "series = df['BodyMass_lb_inter'].reset_index()\n",
    "\n",
    "#loop through series and move the interpolated weight one index (data) up\n",
    "for ind in range(0,len(series)-1):\n",
    "    series.loc[ind, 'BodyMass_lb_inter'] = series.loc[ind+1, 'BodyMass_lb_inter']\n",
    "\n",
    "#make the last value Nan    \n",
    "series.loc[ind+1, 'BodyMass_lb_inter'] = np.NaN\n",
    "\n",
    "#re-stablish date index\n",
    "series.set_index('date', inplace = True)\n",
    "\n",
    "#create new feature in df to represent the new lagged body mass\n",
    "df['BodyMass_lb_inter'] = series['BodyMass_lb_inter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create the weight difference in a new column\n",
    "df['BodyMass_lb_diff'] = df['BodyMass_lb_inter'].diff() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide the first differenced entry\n",
    "df.iloc[0,len(df.columns)-1] = df.iloc[0,len(df.columns)-2] - df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(15,7)\n",
    "\n",
    "plt.plot(df['BodyMass_lb_diff'], color='blue', label = 'Weight Diff')\n",
    "\n",
    "plt.title('Weigh-In Data Difference (lbs)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.dropna(subset=['BodyMass_lb_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SepOct = df['2023-08-23': '2023-10-24']\n",
    "#NovDec = df['2023-10-31': '2023-12-23']\n",
    "#Jan = df['2024-01-01': '2024-02-05']\n",
    "#FebMar = df['2024-02-24': '2024-03-05']\n",
    "\n",
    "#new_df = pd.concat([SepOct, NovDec, Jan, FebMar])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['BodyMass_lb_diff'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have differenced the data, and have nothing null, let's go ahead and test for Dickey-Fuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = adfuller(new_df['BodyMass_lb_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Print Dickey-Fuller test results\n",
    "print('Results of Dickey-Fuller Test: \\n')\n",
    "\n",
    "dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', \n",
    "                                             '#Lags Used', 'Number of Observations Used'])\n",
    "for key, value in dftest[4].items():\n",
    "    dfoutput['Critical Value (%s)'%key] = value\n",
    "print(dfoutput)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good. The differenced data appears stationary. Let's see how the decomposed time looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "pd.plotting.autocorrelation_plot(new_df['BodyMass_lb_inter']);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 5\n",
    "\n",
    "plot_pacf(new_df['BodyMass_lb_inter'], lags=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both plots look pretty stationary. So that's great. Also, both plots trail off with time. This is a good sign that these functions are a good candidate for AutoRegressive (AR) and Moving Average (MA) Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARMA Analysis\n",
    "We've confirmed our data is stationary. We observed the PACF And ACF plots and understood that both trail off wtih time. This means our weigh-in data is a good candidate for both AR and MA. To do this. We're going to utilize our original weigh-in data. There's plenty of missing data, but luckily our ARIMA works with missing data. We also no that 1st order differencing made our data stationary, so we can jump straight to that when we check for ARMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['BodyMass_lb_inter'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets. Let's do an 80/20 split\n",
    "#SepOct = new_df['2023-08-25': '2023-10-24']\n",
    "#NovDec = new_df['2023-10-31': '2023-12-23']\n",
    "#Jan = new_df['2024-01-01': '2024-02-05']\n",
    "#FebMar = new_df['2024-02-24': '2024-03-05']\n",
    "\n",
    "train = new_df['2023-08-25': '2023-12-23']['BodyMass_lb_inter']\n",
    "test = new_df['2023-12-24': '2024-03-05']['BodyMass_lb_inter']\n",
    "\n",
    "train_len = len(train)\n",
    "test_len = len(test)\n",
    "\n",
    "# walk-forward validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "\n",
    "model = auto_arima(new_df['BodyMass_lb_inter'], seasonal=False, m=0, stepwise=True)\n",
    "\n",
    "# Get the best ARIMA model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... we run the auto and we find the most accurate ARIMA arrangement, which is 1st order lagged on both the moving average and the autoregressed term. This makes sense - we already determined that the differenced data was stationary, and it appears that we also care about both AR and MA. To run the auto, we had to use some synthetic data, but we can also utilize a manual check. We can also check the interpolated data and the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ARIMA\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Instantiate an AR(1) model to the simulated data\n",
    "mod_arma_raw = ARIMA(new_df['BodyMass_lb_inter'], order=(1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to data\n",
    "res_arma_raw = mod_arma_raw.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out summary information on the fit\n",
    "print(res_arma_raw.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it appears we got a significantly more accurate model, which also dropped out the Y-intercept term. We also have statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an MA(1) model to the simulated data\n",
    "mod_arma_inter = ARIMA(new_df['BodyMass_lb_inter'], order=(1,1,1))\n",
    "\n",
    "# Fit the model to data\n",
    "res_arma_inter = mod_arma_inter.fit()\n",
    "\n",
    "# Print out summary information on the fit\n",
    "print(res_arma_inter.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. On our first check with got an AIC/BIC in the high 400s, but coefficients with high high statistical confidence. This was using the raw, uninterpolated data. Let's see how this look with the interpolated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arma_raw_resid = pd.Series(res_arma_raw.resid)\n",
    "arma_raw_resid.drop('2023-08-24', axis = 0, inplace = True)\n",
    "\n",
    "arma_inter_resid = pd.Series(res_arma_inter.resid)\n",
    "arma_inter_resid.drop('2023-08-24', axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arma_inter_resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(15,7)\n",
    "\n",
    "plt.plot(arma_raw_resid, color='blue', label = 'Predictions - Actual Differences')\n",
    "plt.plot(arma_inter_resid, color='red', label = 'Predictions - Interpolated Differences')\n",
    "plt.plot(new_df['BodyMass_lb'].diff(), color='green', label = 'Weight Diff')\n",
    "\n",
    "plt.title('Actual vs Predicted Weigh-In Data (lbs)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arma_inter_resid.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... we can see that the predictions with interpolated differences, do a good job of sticking to the general peaks. We don't visually see much of a drop off in accuracy, even though our model tells us otherwise. I'm inclined to utilize the residuals from the interpolated data as our error.\n",
    "\n",
    "But what does this mean about our weight data?  It means that are both enourages and fights whatever weight difference we experienced? It's almost as if a part of our metabolism wants to continue a trend and another part is trying to course correct. Anecdotally, there are stories about how really in shape super athletes will metablolize excess carbs/fat as opposed to storing them as fat. It's almost if the body knows our behavior, and wants to continue it. Why store fat on an athlete that is in burn mode. Let's just store as glycogen or, get rid of it. On the one hand, the course correction side, perhaps there's a mechanism in our body that's continually trying to use/store/release all of the calolories that are body consumes. Perhaps as very finite corrections. As if the body says, \"Well, I thought I was going to burn X amount of calories, but I only burned Y. So tomorrow, I'll slow down and metabolize less.\" The course correction could also swing the other way - if too much weight gain, the body could metabolize more.\n",
    "\n",
    "But how does that account for weight loss. That's where our trend of moving average comes in (also, the drift in the random walk model). Herein lies a conundrum, our original weight difference numbers passed the test for stationality, but there is a slight trend in the data (-.12). This is approximately 0.12 lbs per day that, on average, of weight loss. It's small enough to not throw off stationality, yet large enough to lose more than 15 lbs in 6 months.\n",
    "\n",
    "Okay, so, back to our diet information. We have two separate errors now from which we can predict some noise. First we have our original weight loss change from day to day. Second, we have the residuals from our ARMA model with which it predict. So... let's do it.\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "So, now that we added have scrubbed our data. We're going to create a few different target variables, all in the name of weight loss. The first, will be just the difference of our weight loss from day to day. The second, will be the residuals from our predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's separate our target and feature columns.\n",
    "#df['diff_inter'] = arma_inter_resid\n",
    "df['BodyMass_lb_raw'] = df['BodyMass_lb']\n",
    "\n",
    "df.drop('BodyMass_lb', axis = 1, inplace = True)\n",
    "df.drop('day', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move the interpolated weight up an index. Let's also drop our Nans on the last row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop NAs from the last row\n",
    "df = df.dropna(subset=['BodyMass_lb_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use this time to make a category to determine if weight loss occurred. This is relatively simple. Let's call it weight loss, and we'll give it a 1, if there's was weight loss, and 0 if there wasn't. In this scenario, even 0 lbs would be the same as weight gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weight_loss'] = df['BodyMass_lb_diff'] < 0.01\n",
    "df['weight_loss'] = df['weight_loss'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_days = pd.DataFrame(df[df['weight_loss'] == 1]['weight_loss'].resample('M').count())\n",
    "weight_days['weight_gain'] = df[df['weight_loss'] == 0]['weight_loss'].resample('M').count()\n",
    "weight_days.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight_days['date'] = weight_days['date'].dt.month\n",
    "weight_days['date'] = weight_days['date'].dt.month_name().str[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_days.set_index('date', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax = weight_days['weight_gain'].plot.bar(color='black', label = 'Weight Gain Days')\n",
    "ax = weight_days['weight_loss'].plot.bar(bottom = weight_days['weight_gain'], color ='red', label = 'Weight Loss Days')\n",
    "\n",
    "ax.set_title('Morning Weigh-In Count')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Days')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the values of blue bars (height)\n",
    "weight_gain = weight_days['weight_gain']\n",
    "\n",
    "# Specify the values of orange bars (height)\n",
    "weight_loss = weight_days['weight_loss']\n",
    "\n",
    "# Position of bars on x-axis\n",
    "ind = np.arange(len(weight_days['weight_gain']))\n",
    "\n",
    "# Figure size\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Width of a bar \n",
    "width = 0.3       \n",
    "\n",
    "# Plotting\n",
    "plt.bar(ind, weight_loss, width, label='Weight Loss Days', color = 'red')\n",
    "plt.bar(ind + width, weight_gain, width, label='Weight Gain Days', color = 'black')\n",
    "\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Days')\n",
    "plt.title('Morning Weigh-In Days by Month')\n",
    "\n",
    "# xticks()\n",
    "# First argument - A list of positions at which ticks should be placed\n",
    "# Second argument -  A list of labels to place at the given locations\n",
    "plt.xticks(ind + width/2, weight_days.index)\n",
    "\n",
    "# Finding the best position for legends and putting it\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_days.loc['totals'] = [weight_days['weight_loss'].sum(), weight_days['weight_gain'].sum()]\n",
    "weight_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "Now that we have all of these feature variables, and we believe we're in good shape. Let's figure out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = df.loc[:,'BodyMass_lb_inter':'weight_loss']\n",
    "features = df.loc[:,'BasalEnergyBurned_Cal':'SleepAnalysis_Awake_hrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets['BodyMass_lb_diff']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! That is pretty good. In fact, it's really where there's no data do we see these big gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_minmax = MinMaxScaler() \n",
    "features_minmax = scaler_minmax.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_std = StandardScaler() \n",
    "features_std = pd.DataFrame(scaler_std.fit_transform(features), columns = features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "#import seaborn as sns\n",
    "corr_check = features_std.corr()\n",
    "corr_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try to analyze the PCA's here\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_1 = PCA(n_components=12)\n",
    "pca_2 = PCA(n_components=24)\n",
    "pca_3 = PCA(n_components=36)\n",
    "\n",
    "principalComponents = pca_1.fit_transform(features_std)\n",
    "principalComponents = pca_2.fit_transform(features_std)\n",
    "principalComponents = pca_3.fit_transform(features_std)\n",
    "\n",
    "print(np.sum(pca_1.explained_variance_ratio_))\n",
    "print(np.sum(pca_2.explained_variance_ratio_))\n",
    "print(np.sum(pca_3.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, okay, so we can maintain about 80% of our data through 12 components, down from 45. At the same time, a lot of correlation (the heat in our correlation map). It's probably a good time to delve into the data a bit more. Previously, we divided our data into dietary, exercise, and sleep. It turns out, we may need to create further subsets. Let's start with dietary. \n",
    "\n",
    "For dietary information, it's useful to think of it in levels. It starts with Level 1 - `DietaryEnergyConsumed_Cal`, from there we go to Level 2 - macronutrients  `DietaryFatTotals_g`, `DietaryCarbohydrates_g`, `DietaryProtein_g`. But fortunately for us, we have, what I call, Level 3 - sub-macronutrients still measured in grams, which includes things like `DietarySugar_g` which is a carbohydrate, and `DietarySaturatedFats_g` which is a fat. Going further, we have micronutrients, or Level 4 - measured in milligrams (or even micrograms) of things like `DietarySodium_mg` and `DietaryCholesterol_mg`.\n",
    "\n",
    "Same with sleep. With sleep, we have level 2 data - REM, Core, Deep. We also have awake hours as well. Level 1 data, if we wanted it, would consist of the total hours of sleep we got. So, if we chose to include only Level 1 diet data in our analysis, it might be better to be consistent with sleep as well. Same with exercise. We have basal and active calories, or Level 2, and we have exercise minutes. Exercise minutes are even a collary category of workout. \n",
    "\n",
    "There's a big correlative overlap between Level 1, 2, & 3. So, we have to make a decision on what we want to include. Given where we are, let's start with Level 1 and go from there.\n",
    "\n",
    "To do that, let's create these categories of sub-data. For sleep, we'll have to feature engineer to add it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's add totals for sleep and energy burned\n",
    "df['SleepAnalysis_AsleepTotal_hrs'] = df['SleepAnalysis_AsleepDeep_hrs'] + df['SleepAnalysis_AsleepCore_hrs'] + df['SleepAnalysis_AsleepREM_hrs']\n",
    "df['TotalEnergyBurned_Cal'] = df['BasalEnergyBurned_Cal'] + df['ActiveEnergyBurned_Cal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all 3 - Level 1 \n",
    "level_1 = ['DietaryEnergyConsumed_Cal', 'TotalEnergyBurned_Cal', 'SleepAnalysis_AsleepTotal_hrs']\n",
    "level_1_diet = ['DietaryEnergyConsumed_Cal']\n",
    "level_1_exer = ['TotalEnergyBurned_Cal']\n",
    "level_1_sleep = ['SleepAnalysis_AsleepTotal_hrs']\n",
    "\n",
    "feature_1 = df[level_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine - Level 2\n",
    "level_2_diet = ['DietaryFatTotal_g', 'DietaryProtein_g', 'DietaryCarbohydrates_g']\n",
    "level_2_exer = ['BasalEnergyBurned_Cal','ActiveEnergyBurned_Cal']\n",
    "level_2_sleep = ['SleepAnalysis_AsleepDeep_hrs','SleepAnalysis_AsleepCore_hrs','SleepAnalysis_AsleepREM_hrs', 'SleepAnalysis_Awake_hrs']\n",
    "level_2 = level_2_diet + level_2_exer + level_2_sleep\n",
    "feature_2 = df[level_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[level_2_diet].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[level_2_exer].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering - let's create some of the categories for dietary 3\n",
    "df['DietaryCarbsResidual_g'] = df['DietaryCarbohydrates_g'] - df['DietarySugar_g'] - df['DietaryFiber_g'] \n",
    "df['DietaryFatsResidual_g'] = df['DietaryFatTotal_g'] - df['DietaryFatMonounsaturated_g'] -  df['DietaryFatPolyunsaturated_g'] - df['DietaryFatSaturated_g'] \n",
    "\n",
    "#let's aggregate the level 3 dietary information\n",
    "level_3_diet_carbs = ['DietaryCarbsResidual_g', 'DietarySugar_g', 'DietaryFiber_g']\n",
    "level_3_diet_fat = ['DietaryFatsResidual_g', 'DietaryFatMonounsaturated_g', 'DietaryFatPolyunsaturated_g', 'DietaryFatSaturated_g']\n",
    "level_3_diet_protein = ['DietaryProtein_g']\n",
    "level_3_diet = level_3_diet_carbs + level_3_diet_fat + level_3_diet_protein\n",
    "\n",
    "#combine - Level 3, please note, there is no level 3 for sleep and exercise, we will reuse level 2 info there\n",
    "level_3 = level_3_diet + level_2_exer + level_2_sleep\n",
    "feature_3 = df[level_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[level_3_diet].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, let's scale the data and redo or correlation matrix, will use both minmax and standard for reference\n",
    "scaler_minmax = MinMaxScaler() \n",
    "feature_1_minmax = pd.DataFrame(scaler_minmax.fit_transform(feature_1), columns = feature_1.columns)\n",
    "feature_1_minmax['date'] = targets['BodyMass_lb_diff'].index\n",
    "feature_1_minmax = feature_1_minmax.set_index('date')\n",
    "\n",
    "scaler_std = StandardScaler() \n",
    "feature_1_std = pd.DataFrame(scaler_std.fit_transform(feature_1), columns = feature_1.columns)\n",
    "feature_1_std['date'] = targets['BodyMass_lb_diff'].index\n",
    "feature_1_std = feature_1_std.set_index('date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "feature_1_std.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_std = StandardScaler() \n",
    "feature_2_std = pd.DataFrame(scaler_std.fit_transform(feature_2), columns = feature_2.columns)\n",
    "feature_2_std['date'] = targets['BodyMass_lb_diff'].index\n",
    "feature_2_std = feature_2_std.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_2_std.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we solved our correlation and components problem... simply by applying domain knowledge and feature engineering. Now, we can run some analysis here.\n",
    "\n",
    "So... which analysis should we use first. The solution is obvious... linear regression. Before we dive in, we should be aware of something in our protocol. The weigh-ins occurred every morning, first thing. They are recorded as weight's for that day. But, much like sleep, the weight recorded that morning is really a reflection of the previous days activities. Or, put it this way, the weight recorded on, say, October 17th as nothing to do with the food, exercise, and sleep on October 17th. As their shown in the data, they're linked. It's more accurate to show the weigh-in occuring on October 17th as the result of behaviors on October 16th. We'll make a new column called \"Lagged Weight\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify X and Y, remembering to drop the last entry as\n",
    "X = feature_1_std\n",
    "y = targets['BodyMass_lb_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "level_1_model = sm.OLS(y, sm.add_constant(X))\n",
    "level_1_results = level_1_model.fit()\n",
    "\n",
    "#print results\n",
    "print(level_1_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1_rolling_2 = feature_1.rolling(2).sum().drop(['2023-08-24'], axis = 0)\n",
    "y.drop('2023-08-24', axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify X and Y, remembering to drop the first entry as\n",
    "X = feature_1_rolling_2\n",
    "\n",
    "#create model\n",
    "level_1_model_minmax = sm.OLS(y, sm.add_constant(X))\n",
    "level_1__minmax_results = level_1_model_minmax.fit()\n",
    "\n",
    "#print results\n",
    "print(level_1__minmax_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, or model got worse! We have a slightly higher AIC/BIC, we have one moving average variable (basically 0). And non of our coefficients are statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1_rolling_3 = feature_1.rolling(3).sum().drop(['2023-08-24', '2023-08-25'], axis = 0)\n",
    "y.drop('2023-08-25', axis = 0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify X and Y, remembering to drop the first entry as\n",
    "X = feature_1_rolling_3\n",
    "\n",
    "#create model\n",
    "level_1_model_minmax = sm.OLS(y, sm.add_constant(X))\n",
    "level_1__minmax_results = level_1_model_minmax.fit()\n",
    "\n",
    "#print results\n",
    "print(level_1__minmax_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_1, targets['BodyMass_lb_diff'], random_state = 243, test_size = .25)\n",
    "\n",
    "# Split the data\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, random_state = 243, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transform the training and test sets\n",
    "scaled_data_train = scaler.fit_transform(X_train_final)\n",
    "scaled_data_val = scaler.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()\n",
    "\n",
    "#we'll start with 10 neurons, and an input shape of 14\n",
    "model_1.add(Dense(12, activation='linear', input_shape=(3,)))\n",
    "model_1.add(Dense(8, activation='linear'))\n",
    "model_1.add(Dense(4, activation='linear'))\n",
    "\n",
    "#output classification layer\n",
    "model_1.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from tensorflow_addons.metrics import RSquare\n",
    "# Compile the model\n",
    "#metric = keras.metrics.R2Score()\n",
    "model_1.compile(loss='mse', optimizer=optimizers.RMSprop(learning_rate=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "results_1  = model_1.fit(scaled_data_train,\n",
    "                   y_train_final,\n",
    "                    epochs=100,\n",
    "                    validation_data=(scaled_data_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_2, targets['BodyMass_lb_diff'], random_state = 243, test_size = .25)\n",
    "\n",
    "# Split the data\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, random_state = 243, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transform the training and test sets\n",
    "scaled_data_train = scaler.fit_transform(X_train_final)\n",
    "scaled_data_val = scaler.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()\n",
    "\n",
    "#we'll start with 10 neurons, and an input shape of 9\n",
    "model_1.add(Dense(12, activation='linear', input_shape=(9,)))\n",
    "model_1.add(Dense(8, activation='linear'))\n",
    "model_1.add(Dense(4, activation='linear'))\n",
    "\n",
    "#output classification layer\n",
    "model_1.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss='mse', optimizer='rmsprop', metrics=[RSquare()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "results_1  = model_1.fit(scaled_data_train,\n",
    "                   y_train_final,\n",
    "                    epochs=100,\n",
    "                    validation_data=(scaled_data_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be getting worse. It may be that we don't have adequate data to determine a link with linear regression. Or, let's try testing using binary classification before we go further. We can use binary classification to use alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_3, targets['BodyMass_lb_diff'], random_state = 243, test_size = .25)\n",
    "\n",
    "# Split the data\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, random_state = 243, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transform the training and test sets\n",
    "scaled_data_train = scaler.fit_transform(X_train_final)\n",
    "scaled_data_val = scaler.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()\n",
    "\n",
    "#we'll start with 10 neurons, and an input shape of 14\n",
    "model_1.add(Dense(12, activation='linear', input_shape=(14,)))\n",
    "model_1.add(Dense(8, activation='linear'))\n",
    "model_1.add(Dense(4, activation='linear'))\n",
    "\n",
    "#output classification layer\n",
    "model_1.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss='mse', optimizer=optimizers.RMSprop(learning_rate=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "results_1  = model_1.fit(scaled_data_train,\n",
    "                   y_train_final,\n",
    "                    epochs=100,\n",
    "                    validation_data=(scaled_data_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial Classification\n",
    "\n",
    "### Baseline Model\n",
    "As we mentioned earlier. Let's see what a model would produce, picking weight loss days at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random module\n",
    "import random\n",
    "\n",
    "#initialize baseline dataframe from the weight loss column\n",
    "baseline = pd.DataFrame(targets['weight_loss'])\n",
    "\n",
    "#create a predictions column that randomly chooses 0 or 1\n",
    "baseline['Predictions'] = np.random.randint(0,1,len(baseline))\n",
    "\n",
    "#create another column which determines which are correct\n",
    "baseline['Correct?'] = (baseline['weight_loss'] == baseline['Predictions'])\n",
    "\n",
    "#count the true and false answers\n",
    "baseline['Correct?'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs = df.groupby('weight_loss').agg(['mean', 'std'])\n",
    "aggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's not surprising. This is basically a 50-50 model (48%), with a few more weight_loss days than weight_gain days. Our model was about 48% accurate. Let "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Neighbors\n",
    "So, let's run through some of the standard algorithms for each level of features.\n",
    "\n",
    "#### Level 1 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_1, df['weight_loss'], random_state = 42, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transform the training and test sets\n",
    "scaled_data_train = scaler.fit_transform(X_train)\n",
    "scaled_data_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# Convert into a DataFrame\n",
    "scaled_df_train = pd.DataFrame(scaled_data_train, columns = feature_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Instantiate KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=11)\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(scaled_data_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "test_preds = clf.predict(scaled_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "# Complete the function\n",
    "def print_metrics(labels, preds):\n",
    "    print(\"Precision Score: {}\".format(precision_score(labels, preds)))\n",
    "    print(\"Recall Score: {}\".format(recall_score(labels, preds)))\n",
    "    print(\"Accuracy Score: {}\".format(accuracy_score(labels, preds)))\n",
    "    print(\"F1 Score: {}\".format(f1_score(labels, preds)))\n",
    "    \n",
    "print_metrics(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = scaler.fit_transform(feature_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1 = cross_val_score(clf, X_transformed, df['weight_loss'], cv=10) #10 fold cross validation\n",
    "scores_1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... Our level one cross-validated accuracy was 63%\n",
    "\n",
    "### Level 2 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_2, df['weight_loss'], random_state = 42, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transform the training and test sets\n",
    "scaled_data_train = scaler.fit_transform(X_train)\n",
    "scaled_data_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# Convert into a DataFrame\n",
    "scaled_df_train = pd.DataFrame(scaled_data_train, columns = feature_2.columns)\n",
    "scaled_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k(X_train, y_train, X_test, y_test, min_k=1, max_k=25):\n",
    "    best_k = 0\n",
    "    best_score = 0.0\n",
    "    for k in range(min_k, max_k+1, 2):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        preds = knn.predict(X_test)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        if f1 > best_score:\n",
    "            best_k = k\n",
    "            best_score = f1\n",
    "    \n",
    "    print(\"Best Value for k: {}\".format(best_k))\n",
    "    print(\"F1-Score: {}\".format(best_score))\n",
    "find_best_k(scaled_data_train, y_train, scaled_data_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_k(scaled_data_train, y_train, scaled_data_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Instantiate KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors = 21)\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(scaled_data_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "test_preds = clf.predict(scaled_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "# Complete the function\n",
    "def print_metrics(labels, preds):\n",
    "    print(\"Precision Score: {}\".format(precision_score(labels, preds)))\n",
    "    print(\"Recall Score: {}\".format(recall_score(labels, preds)))\n",
    "    print(\"Accuracy Score: {}\".format(accuracy_score(labels, preds)))\n",
    "    print(\"F1 Score: {}\".format(f1_score(labels, preds)))\n",
    "    \n",
    "print_metrics(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, it looks like we have some decent accurate right out of the gate. Let's check the validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = scaler.fit_transform(feature_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores_2 = cross_val_score(clf, X_transformed, df['weight_loss'], cv=10) #10 fold cross validation\n",
    "scores_2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we got an accuracy of 60%, which is lower than our previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN - Level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_3, df['weight_loss'], random_state = 42, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transform the training and test sets\n",
    "scaled_data_train = scaler.fit_transform(X_train)\n",
    "scaled_data_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k(X_train, y_train, X_test, y_test, min_k=1, max_k=25):\n",
    "    best_k = 0\n",
    "    best_score = 0.0\n",
    "    for k in range(min_k, max_k+1, 2):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        preds = knn.predict(X_test)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        if f1 > best_score:\n",
    "            best_k = k\n",
    "            best_score = f1\n",
    "    \n",
    "    print(\"Best Value for k: {}\".format(best_k))\n",
    "    print(\"F1-Score: {}\".format(best_score))\n",
    "find_best_k(scaled_data_train, y_train, scaled_data_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(scaled_data_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "test_preds = clf.predict(scaled_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = scaler.fit_transform(feature_3)\n",
    "scores_3 = cross_val_score(clf, X_transformed, df['weight_loss'], cv=10) #10 fold cross validation\n",
    "scores_3.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 56% Accurate on Test Data, so, it turns out we were most accurate with our high level 1 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Level 1 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_1, df['weight_loss'], random_state = 42, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transform the training and test sets\n",
    "scaled_data_train = scaler.fit_transform(X_train)\n",
    "scaled_data_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1E12, solver='lbfgs')\n",
    "model_log = logreg.fit(scaled_data_train, y_train)\n",
    "model_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = scaler.fit_transform(feature_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1 = cross_val_score(logreg, X_transformed, df['weight_loss'], cv=20) #10 fold cross validation\n",
    "scores_1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70% Accurate on cross-validated data. That's pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Level 2 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_2, df['weight_loss'], random_state = 42, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transform the training and test sets\n",
    "scaled_data_train = scaler.fit_transform(X_train)\n",
    "scaled_data_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(fit_intercept=False, C=1E12, solver='lbfgs')\n",
    "model_log = logreg.fit(scaled_data_train, y_train)\n",
    "model_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = scaler.fit_transform(feature_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_2 = cross_val_score(logreg, X_transformed, df['weight_loss'], cv=20) #10 fold cross validation\n",
    "scores_2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly 61% accurate when cross-validated. So far... the Logistic Regression with level 1 features are giving us our best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 3 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_3, df['weight_loss'], random_state = 42, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transform the training and test sets\n",
    "scaled_data_train = scaler.fit_transform(X_train)\n",
    "scaled_data_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(fit_intercept=False, C=1E12, solver='lbfgs')\n",
    "model_log = logreg.fit(scaled_data_train, y_train)\n",
    "model_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = scaler.fit_transform(feature_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_3 = cross_val_score(logreg, X_transformed, df['weight_loss'], cv=20) #10 fold cross validation\n",
    "scores_3.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, level 3 was 64% accurate. This is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's try decision tree\n",
    "### Decision Tree - Level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_1, df['weight_loss'], random_state = 42, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [1, 2, 5, 10],\n",
    "    'min_samples_split': [1, 5, 10, 20]\n",
    "}\n",
    "\n",
    "gs_tree = GridSearchCV(clf, param_grid, cv=20)\n",
    "gs_tree.fit(X_train, y_train)\n",
    "\n",
    "gs_tree.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier (criterion='gini', max_depth = 1, min_samples_split = 5, random_state = 42)\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "test_preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cross_val_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7831a4e3f6ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#10 fold cross validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mscores_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cross_val_score' is not defined"
     ]
    }
   ],
   "source": [
    "scores_1 = cross_val_score(clf, feature_1, df['weight_loss'], cv=20) #10 fold cross validation\n",
    "scores_1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    n_features = X_train.shape[1]\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), X_train.columns.values) \n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.ylabel('Feature')\n",
    "\n",
    "plot_feature_importances(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OKay, we got an accuracy of 68%. This is not bad on feature 1 data.\n",
    "### Decision Tree - Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_2, df['weight_loss'], random_state = 142, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [1, 2, 5, 10],\n",
    "    'min_samples_split': [1, 5, 10, 20]\n",
    "}\n",
    "\n",
    "gs_tree = GridSearchCV(clf, param_grid, cv=20)\n",
    "gs_tree.fit(X_train, y_train)\n",
    "\n",
    "gs_tree.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier (criterion='gini', max_depth = 1, min_samples_split = 5, random_state = 142)\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "test_preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_2 = cross_val_score(clf, feature_2, df['weight_loss'], cv=20) #10 fold cross validation\n",
    "scores_2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    n_features = X_train.shape[1]\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), X_train.columns.values) \n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.ylabel('Feature')\n",
    "\n",
    "plot_feature_importances(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1, figsize = (3,3), dpi=300)\n",
    "tree.plot_tree(clf,\n",
    "               feature_names = X_train.columns, \n",
    "               class_names=np.unique(df['weight_loss']).astype('str'),\n",
    "               filled = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvement, we've matched our best score with 70% Accuracy on our model. Let's see...\n",
    "### Level 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_3, df['weight_loss'], random_state = 142, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [1, 2, 5, 10],\n",
    "    'min_samples_split': [1, 5, 10, 20]\n",
    "}\n",
    "\n",
    "gs_tree = GridSearchCV(clf, param_grid, cv=20)\n",
    "gs_tree.fit(X_train, y_train)\n",
    "\n",
    "gs_tree.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier (criterion='gini', max_depth = 10, min_samples_split = 5, random_state = 142)\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "test_preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores_3 = cross_val_score(clf, feature_3, df['weight_loss'], cv=20) #10 fold cross validation\n",
    "scores_3.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    n_features = X_train.shape[1]\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), X_train.columns.values) \n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.ylabel('Feature')\n",
    "\n",
    "plot_feature_importances(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting... our accuracy went down with the more\n",
    "\n",
    "### Naive Bayes \n",
    "### Level 1\n",
    "theorem and see if that helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_1, df['weight_loss'], random_state = 42, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Guassian\n",
    "GNB = GaussianNB()\n",
    "GNB.fit(X_train,y_train)\n",
    "#\n",
    "# Predict for test set\n",
    "#\n",
    "y_pred = GNB.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = scaler.fit_transform(feature_1)\n",
    "scores_1 = cross_val_score(GNB, X_transformed, df['weight_loss'], cv=20) #10 fold cross validation\n",
    "scores_1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of 65%. Not as good as our previous answers but not bad.\n",
    "### Naive Bayes \n",
    "### Level 2\n",
    "theorem and see if that helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_2, df['weight_loss'], random_state = 42, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guassian\n",
    "GNB = GaussianNB()\n",
    "GNB.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = scaler.fit_transform(feature_2)\n",
    "scores_2 = cross_val_score(GNB, X_transformed, df['weight_loss'], cv=20) #10 fold cross validation\n",
    "scores_2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of 59%. Not as good as our previous answers but not bad.\n",
    "### Naive Bayes \n",
    "### Level 3\n",
    "theorem and see if that helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_3, df['weight_loss'], random_state = 42, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guassian\n",
    "GNB = GaussianNB()\n",
    "GNB.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = scaler.fit_transform(feature_3)\n",
    "scores_3 = cross_val_score(GNB, feature_3, df['weight_loss'], cv=20) #10 fold cross validation\n",
    "scores_3.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so that didn't do much either. Our Naive Bayes first model yeilded a result of .53. So... not that good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we got 70% true with Gaussian. This is, arguably our best result. Something to keep in mind as we go forward.\n",
    "\n",
    "### SVM\n",
    "#### Level 1\n",
    "Let's do an analysis of linear SVM. We'll avoid non-linear for now as the level of complexity might be too high for a binomial classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_1, df['weight_loss'], random_state = 142, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "X_train_transformed = std.fit_transform(X_train)\n",
    "X_test_transformed = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "#r_range = np.array([0.01, 1, 10])  \n",
    "#gamma_range = np.array([0.001, 0.01, 0.1]) \n",
    "#param_grid = dict(gamma=gamma_range, coef0=r_range)\n",
    "#details = []\n",
    "#for gamma in gamma_range:\n",
    "#     for r in r_range:\n",
    "#        clf = svm.SVC(kernel='linear', coef0=r , gamma=gamma)\n",
    "#        clf.fit(X_train_transformed, y_train)\n",
    "#        score = clf.score(X_test_transformed, y_test)\n",
    "#        details.append((r, gamma, clf, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = std.fit_transform(feature_1)\n",
    "\n",
    "scores_1 = cross_val_score(clf, X_transformed, df['weight_loss'], cv=20) #10 fold cross validation\n",
    "scores_1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wow, let's plot the feature importance\n",
    "pd.Series(clf.coef_[0], index=X_train.columns).nlargest(11).plot(kind='barh', title='Weight Loss Contributors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "65% on an SVM model is good. Let's see if we can improve it.\n",
    "### SVM\n",
    "#### Level 2\n",
    "Let's do an analysis of linear SVM. We'll avoid non-linear for now as the level of complexity might be too high for a binomial classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_2, df['weight_loss'], random_state = 142, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "X_train_transformed = std.fit_transform(X_train)\n",
    "X_test_transformed = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = std.fit_transform(feature_2)\n",
    "\n",
    "scores_2 = cross_val_score(clf, X_transformed, df['weight_loss'], cv=20) #10 fold cross validation\n",
    "scores_2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wow, let's plot the feature importance\n",
    "pd.Series(clf.coef_[0], index=X_train.columns).nlargest(11).plot(kind='barh', title='Weight Loss Contributors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "65% on an SVM model is good. Let's see if we can improve it.\n",
    "### SVM\n",
    "#### Level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_3, df['weight_loss'], random_state = 142, test_size = .25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so after a few SVM rounds, it appears we don't have great luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "X_train_transformed = std.fit_transform(X_train)\n",
    "X_test_transformed = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = std.fit_transform(feature_3)\n",
    "\n",
    "scores_2 = cross_val_score(clf, X_transformed, df['weight_loss'], cv=20) #10 fold cross validation\n",
    "scores_2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wow, let's plot the feature importance\n",
    "pd.Series(clf.coef_[0], index=X_train.columns).nlargest(14).plot(kind='barh', title='Weight Loss Contributors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "#from sklearn.preprocessing import StandardScaler, LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train_total, X_test, y_train_total, y_test = train_test_split(feature_1, df['weight_loss'], random_state = 124, test_size = .25)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_total, y_train_total, random_state = 124, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transform the training and test sets\n",
    "scaled_data_train = scaler.fit_transform(X_train)\n",
    "scaled_data_val = scaler.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()\n",
    "\n",
    "#we'll start with 10 neurons, and an input shape of 14\n",
    "model_1.add(Dense(12, activation='relu', input_shape=(3,)))\n",
    "model_1.add(Dense(6, activation='tanh'))\n",
    "model_1.add(Dense(2, activation='relu'))\n",
    "\n",
    "#output classification layer\n",
    "model_1.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "# Compile the model\n",
    "model_1.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "results_1  = model_1.fit(scaled_data_train,\n",
    "                    y_train,\n",
    "                    epochs=100,\n",
    "                    validation_data=(scaled_data_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we've done a fair bit of analysis here. We've used some traditional machine learning algorithms and neural networks, and we haven't exceeded 62-65% accuracy. Some of our best models were KNN, Logisitic Regression, SVM. Our worst models were Linear Regression, Naive Bayes, and Decision Tree. Because we have only numeric data, it's no surprised that Decision Tree performed poorly. The accuracy on Linear Regression was particularly bad. For now, we'll abandoned our goals of finding a combined linear regression model with time response, and just focus on classification of weight loss and weight gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we optimize the model? A helpful analysis is there on our SVM plot. Let's look at that again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_test = scaler.fit_transform(X_test)\n",
    "\n",
    "score = model_1.evaluate(scaled_data_test, y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so this overfit our data. It's... a lot of analysis for only 3 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check level 2 testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train_total, X_test, y_train_total, y_test = train_test_split(feature_2, df['weight_loss'], random_state = 124, test_size = .25)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_total, y_train_total, random_state = 124, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transform the training and test sets\n",
    "scaled_data_train = scaler.fit_transform(X_train)\n",
    "scaled_data_val = scaler.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "\n",
    "#we'll start with 10 neurons, and an input shape of 14\n",
    "model_2.add(Dense(12, activation='tanh', input_shape=(len(X_train.columns),)))\n",
    "model_2.add(Dense(8, activation='tanh'))\n",
    "model_2.add(Dense(4, activation='tanh'))\n",
    "\n",
    "#output classification layer\n",
    "model_2.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "# Compile the model\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "results_2  = model_2.fit(scaled_data_train,\n",
    "                    y_train,\n",
    "                    epochs=150,\n",
    "                    validation_data=(scaled_data_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_results(results):\n",
    "    history = results.history\n",
    "    plt.figure()\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.plot(history['loss'])\n",
    "    plt.legend(['val_loss', 'loss'])\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(history['val_acc'])\n",
    "    plt.plot(history['acc'])\n",
    "    plt.legend(['val_acc', 'acc'])\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_test = scaler.fit_transform(X_test)\n",
    "\n",
    "score = model_2.evaluate(scaled_data_test, y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good either. It appears this was overfit on both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check level 3 testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train_total, X_test, y_train_total, y_test = train_test_split(feature_3, df['weight_loss'], random_state = 124, test_size = .25)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_total, y_train_total, random_state = 124, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transform the training and test sets\n",
    "scaled_data_train = scaler.fit_transform(X_train)\n",
    "scaled_data_val = scaler.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Sequential()\n",
    "\n",
    "#we'll start with 10 neurons, and an input shape of 14\n",
    "model_3.add(Dense(12, activation='tanh', input_shape=(len(X_train.columns),)))\n",
    "model_3.add(Dense(8, activation='tanh'))\n",
    "model_3.add(Dense(4, activation='tanh'))\n",
    "\n",
    "#output classification layer\n",
    "model_3.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "# Compile the model\n",
    "model_3.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "results_3  = model_3.fit(scaled_data_train,\n",
    "                    y_train,\n",
    "                    epochs=100,\n",
    "                    validation_data=(scaled_data_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_results(results):\n",
    "    history = results.history\n",
    "    plt.figure()\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.plot(history['loss'])\n",
    "    plt.legend(['val_loss', 'loss'])\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(history['val_acc'])\n",
    "    plt.plot(history['acc'])\n",
    "    plt.legend(['val_acc', 'acc'])\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_test = scaler.fit_transform(X_test)\n",
    "\n",
    "score = model_3.evaluate(scaled_data_test, y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... it looks as though our best models our the Decision Tree with Level 2 data, and Logistic Regression with Level 1 data.\n",
    "Let's go ahead and iterate on those two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we achieved, 61% accuracy on test data. It's not quite what we achieved with our SVM and decision tree models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. So it turns out this model was perfect at epock 100. So let's stop the model at epoch 100 and then predict on our test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the importance feature above we can tell a few things.\n",
    "\n",
    "Surprises:\n",
    "1. The most important feature is Basal Energy Burned. This feature essentially tells us how heavy we are, meaning, it looks like our existing weight is the biggest predictor of whether or not we will lose weight the next day.\n",
    "\n",
    "2. Protein is the second largest negative factor contributing to weight loss. We hear a lot about how we need more protein in our diet. It contributes to building and maintaining muscle and organ function. It's possible that weight loss is more than just fat and stored carbohydrates. It's also about losing muscle.\n",
    "\n",
    "3. Residual fats (characterized as all fats not saturated, monounsaturated, and polyunsaturated) are a small contributor to weight loss. We hear sometimes about \"healthy fats\" but interesting to see it as a 3rd largest contributor to weight loss.\n",
    "\n",
    "4. Active Calorie burned was a contributing factor too... weight gain? It's incredibly small, so perhaps with more data, we can get a different result. But regardless, it seems that active calorie burn was insignificant, or even a negative coefficient, to weight loss.\n",
    "\n",
    "Expected:\n",
    "1. REM sleep and Core Sleep both factor into some weight loss, however Deep sleep factors slightly against. The deep sleep coefficient is so small, that it seems as though Total sleep hours would be a more significant factor for next day weight loss.\n",
    "\n",
    "2. Dietary Fiber is the second most important factor contributing to weight loss. This is interesting because we here that having lots of fiber in your diet is important.\n",
    "\n",
    "3. Sugar is the largest negative factor contributing to weight loss. This confirms what we've heard for a while. Interesting to see it here.\n",
    "\n",
    "4. Saturated and Polyunsaturated fats were negative contributors to weight loss, with polyunsaturated fats being the largest contributor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps. Well, we can combine the sleep into one category, that would simplify the analysis, especially from a PCA perspective. We can also add some rolling sums. As we mentioned before, we didn't have much luck doing a time response strictly with the weight data, but perhaps we could include both rolling sums of the data, as well as an indication if there was weight loss the dat before our after.\n",
    "\n",
    "So... let's resume Decision Tree, Logistic Regression, linear SVM \n",
    "\n",
    "First, we'll consolidate the sleep data - we'll leave the hours awake data out, \n",
    "Second, we'll add a 2 day rolling average to each of the numbers.\n",
    "Third, we'll add a previous day weight loss component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sleep Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create our dataset\n",
    "sleep_consolidation = df[level_3_diet + level_2_exer + level_1_sleep]\n",
    "targets = df['weight_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n",
    "okay, let's run a KNN model with updated sleep and see if there's any budge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sleep_consolidation, targets, test_size=0.25, random_state=24)\n",
    "\n",
    "# Standardize the data\n",
    "std = StandardScaler()\n",
    "X_train_transformed = std.fit_transform(X_train)\n",
    "X_test_transformed = std.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's search for the best K for our algorithm\n",
    "def find_best_k(X_train, y_train, X_test, y_test, min_k=1, max_k=25):\n",
    "    best_k = 0\n",
    "    best_score = 0.0\n",
    "    for k in range(min_k, max_k+1, 2):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        preds = knn.predict(X_test)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        if f1 > best_score:\n",
    "            best_k = k\n",
    "            best_score = f1\n",
    "    \n",
    "    print(\"Best Value for k: {}\".format(best_k))\n",
    "    print(\"F1-Score: {}\".format(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_k(X_train_transformed, y_train, X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=11)\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "test_preds = clf.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(labels, preds):\n",
    "    print(\"Precision Score: {}\".format(precision_score(labels, preds)))\n",
    "    print(\"Recall Score: {}\".format(recall_score(labels, preds)))\n",
    "    print(\"Accuracy Score: {}\".format(accuracy_score(labels, preds)))\n",
    "    print(\"F1 Score: {}\".format(f1_score(labels, preds)))\n",
    "    \n",
    "print_metrics(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's cross-validate\n",
    "X_transformed = std.transform(sleep_consolidation)\n",
    "\n",
    "scores = cross_val_score(clf, X_transformed, targets, cv=10)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sleep_consolidation, targets, test_size=0.25, random_state=24)\n",
    "\n",
    "# Standardize the data\n",
    "std = StandardScaler()\n",
    "X_train_transformed = std.fit_transform(X_train)\n",
    "X_test_transformed = std.transform(X_test)\n",
    "\n",
    "X_transformed = std.transform(sleep_consolidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept=False, C=100, solver='liblinear')\n",
    "model_log = logreg.fit(X_train_transformed, y_train)\n",
    "model_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = model_log.predict(X_train_transformed)\n",
    "\n",
    "train_residuals = np.abs(y_train - y_hat_train)\n",
    "print(pd.Series(train_residuals, name=\"Residuals (counts)\").value_counts())\n",
    "print()\n",
    "print(pd.Series(train_residuals, name=\"Residuals (proportions)\").value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = model_log.predict(X_test_transformed)\n",
    "\n",
    "test_residuals = np.abs(y_test - y_hat_test)\n",
    "print(pd.Series(test_residuals, name=\"Residuals (counts)\").value_counts())\n",
    "print()\n",
    "print(pd.Series(test_residuals, name=\"Residuals (proportions)\").value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's cross-validate\n",
    "scores = cross_val_score(model_log, X_transformed, targets, cv=10)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "65%   mean accuracy on cross-validation is not very good.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "#X_train, X_test, y_train, y_test = train_test_split(sleep_consolidation, targets, random_state = 42, test_size = .99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "std = StandardScaler()\n",
    "X_train_transformed = std.fit_transform(sleep_consolidation)\n",
    "#X_test_transformed = std.transform(X_test)\n",
    "\n",
    "X_transformed = std.transform(sleep_consolidation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm = svm.SVC(kernel='linear')\n",
    "svm.fit(X_train_transformed, targets)\n",
    "\n",
    "#svm.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(svm, X_transformed, targets, cv=15)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wow, let's plot the feature importance\n",
    "pd.Series(svm.coef_[0], index=sleep_consolidation.columns).nlargest(11).plot(kind='barh', title='                         <----- Weight Gain | Weight Loss ----->')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we were able to 67% accuracy on mean cross-validation. This is an improvement. Additionally, once we consolidated the sleep factors (and removed awake time), we see a decent increase in the contribution of Dietary Sugar. The active calorie burned stayed relatively small, almost nonexistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(sleep_consolidation, targets, random_state = 243, test_size = .25)\n",
    "\n",
    "# Split the data\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, random_state = 243, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transform the training and test sets\n",
    "X_train_transform = scaler.fit_transform(X_train_final)\n",
    "X_val_transform = scaler.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()\n",
    "\n",
    "#we'll start with 10 neurons, and an input shape of 14\n",
    "model_1.add(Dense(12, activation='relu', input_shape=(11,)))\n",
    "model_1.add(Dense(8, activation='relu'))\n",
    "model_1.add(Dense(4, activation='relu'))\n",
    "\n",
    "#output classification layer\n",
    "model_1.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "# Compile the model\n",
    "model_1.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "results_1  = model_1.fit(X_train_transform,\n",
    "                    y_train_final,\n",
    "                    epochs=75,\n",
    "                    validation_data=(X_val_transform, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training and test sets\n",
    "X_train_transform_1 = scaler.fit_transform(X_train)\n",
    "X_test_transform = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model_1.evaluate(X_train_transform_1, y_train)\n",
    "print('----------')\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model_1.evaluate(X_test_transform, y_test)\n",
    "print('----------')\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we don't have much improvement here. In fact, we got a decrease from our SVM model, which, so far, has the best output.\n",
    "\n",
    "But let's continue with our modeling. Let's do a two day rolling sum of the all of the features and see if that changes anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Day Rolling Sum\n",
    "Previously we created a sleep_consolidation set, that looked something like this.\n",
    "`sleep_consolidation = df[level_3_diet + level_2_exer + level_1_sleep]`\n",
    "\n",
    "so, let's take our new df, and\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create our dataset\n",
    "newdf_roll_sum_2 = sleep_consolidation.rolling(2).sum().drop(['2023-08-24'], axis = 0)\n",
    "#y.drop('2023-08-25', axis = 0,inplace = True)\n",
    "newdf_roll_sum_2\n",
    "\n",
    "targets_roll_sum_2 = targets.drop(['2023-08-24'], axis = 0)\n",
    "targets_roll_sum_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(newdf_roll_sum_2, targets_roll_sum_2, test_size=0.25, random_state=24)\n",
    "\n",
    "# Standardize the data\n",
    "std = StandardScaler()\n",
    "X_train_transformed = std.fit_transform(X_train)\n",
    "X_test_transformed = std.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#let's search for the best K for our algorithm\n",
    "find_best_k(X_train_transformed, y_train, X_test_transformed, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=11)\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "test_preds = clf.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's cross-validate\n",
    "X_transformed = std.transform(newdf_roll_sum_2)\n",
    "\n",
    "scores = cross_val_score(clf, X_transformed, targets_roll_sum_2, cv=10)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(newdf_roll_sum_2, targets_roll_sum_2, test_size=0.25, random_state=24)\n",
    "\n",
    "# Standardize the data\n",
    "std = StandardScaler()\n",
    "X_train_transformed = std.fit_transform(X_train)\n",
    "X_test_transformed = std.transform(X_test)\n",
    "\n",
    "X_transformed = std.transform(newdf_roll_sum_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1E12, solver='liblinear')\n",
    "model_log = logreg.fit(X_train_transformed, y_train)\n",
    "model_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's cross-validate\n",
    "scores = cross_val_score(model_log, X_transformed, targets_roll_sum_2, cv=10)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(newdf_roll_sum_2, targets_roll_sum_2, test_size=0.25, random_state=24)\n",
    "\n",
    "# Standardize the data\n",
    "std = StandardScaler()\n",
    "X_train_transformed = std.fit_transform(X_train)\n",
    "X_test_transformed = std.transform(X_test)\n",
    "\n",
    "X_transformed = std.transform(newdf_roll_sum_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm = svm.SVC(kernel='linear')\n",
    "svm.fit(X_train_transformed, y_train)\n",
    "\n",
    "svm.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(svm, newdf_roll_sum_2, targets_roll_sum_2, cv=15)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wow, let's plot the feature importance\n",
    "pd.Series(svm.coef_[0], index=X_train.columns).nlargest(14).plot(kind='barh', title='Weight Loss Contributors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(newdf_roll_sum_2, targets_roll_sum_2, random_state = 24, test_size = .15)\n",
    "\n",
    "# Split the data\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, random_state = 24, test_size = .15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transform the training and test sets\n",
    "X_train_transform = scaler.fit_transform(X_train_final)\n",
    "X_val_transform = scaler.fit_transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()\n",
    "\n",
    "#we'll start with 10 neurons, and an input shape of 14\n",
    "model_1.add(Dense(12, activation='tanh', input_shape=(11,)))\n",
    "model_1.add(Dense(8, activation='tanh'))\n",
    "model_1.add(Dense(4, activation='tanh'))\n",
    "\n",
    "#output classification layer\n",
    "model_1.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_1.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "results_1  = model_1.fit(X_train_transform,\n",
    "                    y_train_final,\n",
    "                    epochs=75,\n",
    "                    validation_data=(X_val_transform, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARY\n",
    "So, we did not see an improvement with the addition of the rolling sum for 2 days. But I still would like to test some sort of time response idea. Let's see if we can add a feature to represent the previous day was weight loss or weight gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add another feature - previous day's weight loss or gain.\n",
    "So, we know that the rolling sum didn't help. Let's return to our previous day weight gain idea. I'm going to circle back to the Decision Tree - Level 2 and Logistic Regression - Level 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARY\n",
    "We downloaded the data, analyzed it, and decided to run some more analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline, \n",
    "let's establish pipelines for each of our tests. We won't necessarily worry about some of our less accurate ones, but we can start with the basics. We've had good results for Naive Bayes, KNN, and SVM. So, let's run a few test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
